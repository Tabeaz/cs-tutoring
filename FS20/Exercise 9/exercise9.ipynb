{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SwsaLxhGYpE4",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Avth2IRtYpxL",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(\"a7_train.csv\")\n",
        "train_df = train_df.iloc[7500:-7500, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LTrezgijY5-2",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(\"a7_test.csv\")\n",
        "test_df = test_df.iloc[7500:-7500, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrRat9noZAvN",
        "colab": {}
      },
      "source": [
        "def calculate_priors(df):\n",
        "  '''\n",
        "  Calculates the probability of given classes, based on how often they appear in a DataFrame.\n",
        "  \n",
        "  Return a dictionary mapping the class to its probability.\n",
        "\n",
        "  Example: The class \"apple\" appears in two rows, the class \"banana\" in three.\n",
        "  The probability for the \"apple\" class is 2/5, the probability for banana is \n",
        "  3/5.\n",
        "  You have to return a dictionary:\n",
        "    {\"apple\": 0.4, \"banana\": 0.6}\n",
        "\n",
        "  :param df: A pandas.DataFrame containing a column \"class\" and a column \"text\". \n",
        "  :return: A dictionary mapping the class to its probability.\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jk6OM3gPZmnB",
        "outputId": "8176bb2f-87a8-4e85-d20d-e25f77688b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "priors = calculate_priors(train_df)\n",
        "print(priors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWkmctLrZOr7",
        "colab": {}
      },
      "source": [
        "def calculate_class_term_frequency(df, classes):\n",
        "  \"\"\"\n",
        "  For each class, calculates the frequency of terms appearing in texts of this class.\n",
        "\n",
        "  In the DataFrame, each row has an entry for \"text\" and one for \"class\".\n",
        "  The values in the \"text\" column are strings (= actual texts).\n",
        "  Your task is to calculate the frequency (= how often a word appears) for each of the classes.\n",
        "\n",
        "  Convert the terms to lowercase.\n",
        "\n",
        "  Example: \n",
        "    There are two texts with the \"apple\" class. \n",
        "    There are three texts with the \"banana\" class.\n",
        "    The word \"fruit\" appears five times in the texts of the \"apple\" class, and \n",
        "    two times for texts of the \"banana\" class.\n",
        "    The total frequency of the word \"fruit\" is 7. The frequency for the \"apple\"\n",
        "    class is 5. The frequency for the \"banana\" class is 2.\n",
        "\n",
        "  :param df: A pandas.DataFrame containing a column \"class\" and a column \"text\".\n",
        "  :param classes: A list of strings containing the possible values for the \"class\" column.\n",
        "\n",
        "  :return:\n",
        "    - A set containing all the terms (list of string)\n",
        "    - a dictionary containing the frequency of terms appearing in texts of each class in the following format:\n",
        "      {class(string): \n",
        "        {term(string): frequency(int), \n",
        "        ...}, \n",
        "      ...}\n",
        "\n",
        "  \"\"\"\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FNRgc2cMhSnE",
        "colab": {}
      },
      "source": [
        "# terms, freqs_per_class = calculate_class_term_frequency(train_df, [\"pos\", \"neg\"])\n",
        "\n",
        "#print(\"The data containts {} differen tword\".format(len(terms)))\n",
        "#print(\"The word 'bad' appears {} times in {} reviews.\".format(freqs_per_class[\"neg\"][\"bad\"], 'negative'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aYjtusnMZU0o"
      },
      "source": [
        "**Conditional Probabilities (with add-1 smoothing)**: $P(w|c) = \\frac{count(w, c) + 1}{count(c) + |V|}$\n",
        "\n",
        "Example: $P(fruit|apple) = 0.3$\n",
        "\n",
        "$count(w, c)$ = How often word $w$ appears in documents of class $c$.\n",
        "\n",
        "$count(c)$ = How many words appear in documents of class $c$.\n",
        "\n",
        "$|V|$ = How many different words exist in the dataset.\n",
        "\n",
        "**Prior**: $P_{prior}(c) = \\frac{N_c}{N}$, where $N$ is the total number of documents and $N_c$ is the number of documents with class $c$\n",
        "\n",
        "**Choosing a class for document $d$**: $argmax_{c \\in C} P(c|d)$ with $P(c|d) = P_{prior}(c) * \\prod_{w \\in d} P(w|c)$\n",
        "\n",
        "You can also refer to the lecture slides where you can find an example calculation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jtz2IcomZYna",
        "colab": {}
      },
      "source": [
        "def calculate_class_term_probs(terms, freqs_per_class):\n",
        "  \"\"\"\n",
        "\n",
        "  Calculates the probability that a term appears in documents of a given class \n",
        "  and returns a nested dictionary containing the probabilities.\n",
        "\n",
        "  Use Laplace (add-1) smoothing.\n",
        "  Return a nested dictionary. The outer dictionary needs map terms to the \n",
        "  inner dictionaries. The inner dictionary maps class labels to probabilities.\n",
        "\n",
        "  -> If the dictionary is accessed with a non-existing key but an existing class\n",
        "  it should return the value 1/|V| ! <--\n",
        "\n",
        "  Example:\n",
        "    dct = {\"fruit\": {\"apple\": 0.3, \"banana\": 0.4}, \"tree\": {\"apple\": 0.5, \"banana\": 0.1}}\n",
        "    foo = dct[\"rock\"][\"apple\"]\n",
        "    foo == 1/2\n",
        "    Two terms exist in total (fruit and tree) so the total vocabulary size is 2.\n",
        "\n",
        "  :param terms: A list of strings with all unique words that appear in the dataset.\n",
        "  :param freqs_per_class: A dictionary containing the frequency of terms appearing in texts of each class.\n",
        "  :return: A nested dictionary contains the terms and classes with the term's appearance probability.\n",
        "  \n",
        "  \"\"\"\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeFX353_hFgZ",
        "colab": {}
      },
      "source": [
        "# cond_probs = calculate_class_term_probs(terms, freqs_per_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJcQnne8bWPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(cond_probs[\"wow\"][\"pos\"])\n",
        "# print(cond_probs[\"sad\"][\"neg\"])\n",
        "\n",
        "# NOTE: this is a word that does not exist in the data. therefore the value\n",
        "# becomes 1/len(terms) - this is what the formula above for laplace smoothing tells us \n",
        "# print(cond_probs[\"pneumonoultramicroscopicsilicovolcanoconiosis\"][\"pos\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JRlVMNfrZcQl",
        "colab": {}
      },
      "source": [
        "def classify(text, priors, probs):\n",
        "  \"\"\"\n",
        "\n",
        "  This function should predict a class for a given text.\n",
        "  Use the priors you calculated in 1a).\n",
        "  Use the probabilities you calculated in 1c).\n",
        "\n",
        "  !!!!!!!\n",
        "  Instead of multiplying the probabilites together you can take the logarithm\n",
        "  of the probabilities and multiply them together. \n",
        "  !!!!!!!\n",
        "\n",
        "  Use the Naive Bayes classification to assign the class.\n",
        "  \n",
        "  :param text: A string contains the text to clasiffy.\n",
        "  :param priors: A dictionary mapping the class to its probability (From TASK 1a).\n",
        "  :param probs: A nested dictionary contains the terms and classes with\n",
        "  the term's appearance probability (From TASK 1c).\n",
        "  :return: A string for the class label predicted.\n",
        "\n",
        "  \"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J09bPDOIg0CZ",
        "outputId": "bd6600bf-7a53-4ed7-9da2-4436682261a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "t1 = \"Today is an awful day.\"\n",
        "pred = classify(t1, priors, cond_probs)\n",
        "print(f\"'{t1}' is {pred}\")\n",
        "t2 = \"Today is a nice day.\"\n",
        "pred = classify(t2, priors, cond_probs)\n",
        "print(f\"'{t2}' is {pred}\")\n",
        "t3 = \"pneumonoultramicroscopicsilicovolcanoconiosis\"\n",
        "pred = classify(t3, priors, cond_probs)\n",
        "print(f\"'{t3}' is {pred}\")\n",
        "t4 = \"This movie was very good and I liked it a lot.\"\n",
        "pred = classify(t4, priors, cond_probs)\n",
        "print(f\"'{t4}' is {pred}\")\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nt1 = \"Today is an awful day.\"\\npred = classify(t1, priors, probs)\\nprint(f\"\\'{t1}\\' is {pred}\")\\nt2 = \"Today is a nice day.\"\\npred = classify(t2, priors, probs)\\nprint(f\"\\'{t2}\\' is {pred}\")\\nt3 = \"pneumonoultramicroscopicsilicovolcanoconiosis\"\\npred = classify(t3, priors, probs)\\nprint(f\"\\'{t3}\\' is {pred}\")\\nt4 = \"This movie was very good and I liked it a lot.\"\\npred = classify(t4, priors, probs)\\nprint(f\"\\'{t4}\\' is {pred}\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMUwhG-5ZfG4",
        "colab": {}
      },
      "source": [
        "def evaluate(test, priors, probs):\n",
        "  \"\"\"\n",
        "  Evaluate your classification.\n",
        "\n",
        "  -> You may assume that the only classes are \"pos\" and \"neg\" ! <--\n",
        "\n",
        "  The function takes a DataFrame to evaluate the classifier from TASK 1d.\n",
        "  For each row in the test dataset, get the actual class from the \"class\" column\n",
        "  and the text from the \"text\" column.\n",
        "  Use your \"classify(text, priors, probs)\" function to get the predicted\n",
        "  class label for each text.\n",
        "\n",
        "  HINT: Count the TP, FP, FN, TN for each of the two classes, for each prediction. \n",
        "\n",
        "  Calculate the following metrics PER CLASS:\n",
        "    - precision\n",
        "    - recall\n",
        "    - f1-measure\n",
        "\n",
        "  :param test: A pandas.DataFrame containing a column \"class\" and a column \"text\".\n",
        "  :param priors: A dictionary mapping the class to its probability (From TASK 1a).\n",
        "  :param probs: A nested dictionary contains the terms and classes with\n",
        "  the term's appearance probability (From TASK 1c).\n",
        "  :return: two data frames\n",
        "    - metrics, which should represent all the key figures for both classes\n",
        "    - df_stat, shich should represent the confusion matrix\n",
        "\n",
        "  \"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1P37lDUUgvFM",
        "colab": {}
      },
      "source": [
        "# metrics, df_stat = evaluate(test_df, priors, cond_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}