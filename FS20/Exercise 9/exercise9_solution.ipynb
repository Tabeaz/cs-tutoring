{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise9_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ChEsXMJMKI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### This exercise was not created by Janick Spirig. It was an old assignment of the fall semester 2019 in the course FCS ####\n",
        "### However, some parts have been modified / simplified for an ideal learning effect ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SwsaLxhGYpE4",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Avth2IRtYpxL",
        "colab": {}
      },
      "source": [
        "# lets load the training data\n",
        "train_df = pd.read_csv(\"a7_train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LTrezgijY5-2",
        "colab": {}
      },
      "source": [
        "# lets load the test data\n",
        "test_df = pd.read_csv(\"a7_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrRat9noZAvN",
        "colab": {}
      },
      "source": [
        "def calculate_priors(df):\n",
        "  '''\n",
        "  Calculates the probability of given classes, based on how often they appear in a DataFrame.\n",
        "  \n",
        "  Return a dictionary mapping the class to its probability.\n",
        "\n",
        "  Example: The class \"apple\" appears in two rows, the class \"banana\" in three.\n",
        "  The probability for the \"apple\" class is 2/5, the probability for banana is \n",
        "  3/5.\n",
        "  You have to return a dictionary:\n",
        "    {\"apple\": 0.4, \"banana\": 0.6}\n",
        "\n",
        "  :param df: A pandas.DataFrame containing a column \"class\" and a column \"text\". \n",
        "  :return: A dictionary mapping the class to its probability.\n",
        "  '''\n",
        "  \n",
        "  # caluclate relative frequencies of the unique values in the class column and return the result as a dict\n",
        "  return df['class'].value_counts(normalize=True).to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jk6OM3gPZmnB",
        "outputId": "38c05926-033a-4507-b8bb-0a242c02fa4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "priors = calculate_priors(train_df)\n",
        "print(priors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'neg': 0.5, 'pos': 0.5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWkmctLrZOr7",
        "colab": {}
      },
      "source": [
        "def calculate_class_term_frequency(df, classes):\n",
        "  \"\"\"\n",
        "  For each class, calculates the frequency of terms appearing in texts of this class.\n",
        "\n",
        "  In the DataFrame, each row has an entry for \"text\" and one for \"class\".\n",
        "  The values in the \"text\" column are strings (= actual texts).\n",
        "  Your task is to calculate the frequency (= how often a word appears) for each of the classes.\n",
        "\n",
        "  Convert the terms to lowercase.\n",
        "\n",
        "  Example: \n",
        "    There are two texts with the \"apple\" class. \n",
        "    There are three texts with the \"banana\" class.\n",
        "    The word \"fruit\" appears five times in the texts of the \"apple\" class, and \n",
        "    two times for texts of the \"banana\" class.\n",
        "    The total frequency of the word \"fruit\" is 7. The frequency for the \"apple\"\n",
        "    class is 5. The frequency for the \"banana\" class is 2.\n",
        "\n",
        "  :param df: A pandas.DataFrame containing a column \"class\" and a column \"text\".\n",
        "  :param classes: A list of strings containing the possible values for the \"class\" column.\n",
        "\n",
        "  :return:\n",
        "    - A set containing all the terms (list of string)\n",
        "    - a dictionary containing the frequency of terms appearing in texts of each class in the following format:\n",
        "      {class(string): \n",
        "        {term(string): frequency(int), \n",
        "        ...}, \n",
        "      ...}\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # create empty result variables\n",
        "  result_set = set()\n",
        "  result_dict = dict()\n",
        "\n",
        "  # add for each class a sub-dictionary to the parent-dictionary\n",
        "  for c in classes:\n",
        "    result_dict[c] = dict()\n",
        "  \n",
        "  # iterate over all movie reviews\n",
        "  for index, row in df.iterrows():\n",
        "\n",
        "    # convert the review to lower-case and split the review into single words\n",
        "    words = row['text'].lower().split(\" \")\n",
        "\n",
        "    # add all the words to the set which holds all unique words, dupliactes are eliminated automatically\n",
        "    [result_set.add(x) for x in words]\n",
        "\n",
        "    # iterate over all words of the review\n",
        "    for word in words: \n",
        "\n",
        "      # check if word already occurred before\n",
        "      if not word in result_dict[row['class']]:\n",
        "\n",
        "        # word appears for the first time\n",
        "        # create new entry in dict and set counter to one\n",
        "        result_dict[row['class']][word] = 1\n",
        "        \n",
        "      else:\n",
        "        # increase the counter by one\n",
        "        result_dict[row['class']][word] += 1\n",
        "  \n",
        "  # return the set with all unique words and the dict with all words and their frequencies per class\n",
        "  return result_set, result_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FNRgc2cMhSnE",
        "outputId": "c4b26c61-8f15-4723-e169-8fe2f5c4d958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "terms, freqs_per_class = calculate_class_term_frequency(train_df, [\"pos\", \"neg\"])\n",
        "print(\"The data containts {} differen tword\".format(len(terms)))\n",
        "print(\"The word 'bad' appears {} times in {} reviews.\".format(freqs_per_class[\"neg\"][\"bad\"], 'negative'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'pos': {}, 'neg': {}}\n",
            "The data containts 143944 differen tword\n",
            "The word 'bad' appears 2160 times in negative reviews.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aYjtusnMZU0o"
      },
      "source": [
        "**Conditional Probabilities (with add-1 smoothing)**: $P(w|c) = \\frac{count(w, c) + 1}{count(c) + |V|}$\n",
        "\n",
        "Example: $P(fruit|apple) = 0.3$\n",
        "\n",
        "$count(w, c)$ = How often word $w$ appears in documents of class $c$.\n",
        "\n",
        "$count(c)$ = How many words appear in documents of class $c$.\n",
        "\n",
        "$|V|$ = How many different words exist in the dataset.\n",
        "\n",
        "**Prior**: $P_{prior}(c) = \\frac{N_c}{N}$, where $N$ is the total number of documents and $N_c$ is the number of documents with class $c$\n",
        "\n",
        "**Choosing a class for document $d$**: $argmax_{c \\in C} P(c|d)$ with $P(c|d) = P_{prior}(c) * \\prod_{w \\in d} P(w|c)$\n",
        "\n",
        "You can also refer to the lecture slides where you can find an example calculation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jtz2IcomZYna",
        "colab": {}
      },
      "source": [
        "def calculate_class_term_probs(terms, freqs_per_class):\n",
        "  \"\"\"\n",
        "  \n",
        "  Calculates the probability that a term appears in documents of a given class \n",
        "  and returns a nested dictionary containing the probabilities.\n",
        "\n",
        "  Use Laplace (add-1) smoothing.\n",
        "  Return a nested dictionary. The outer dictionary needs map terms to the \n",
        "  inner dictionaries. The inner dictionary maps class labels to probabilities.\n",
        "\n",
        "  -> If the dictionary is accessed with a non-existing key but an existing class\n",
        "  it should return the value 1/|V| ! <--\n",
        "\n",
        "  Example:\n",
        "    dct = {\"fruit\": {\"apple\": 0.3, \"banana\": 0.4}, \"tree\": {\"apple\": 0.5, \"banana\": 0.1}}\n",
        "    foo = dct[\"rock\"][\"apple\"]\n",
        "    foo == 1/2\n",
        "    Two terms exist in total (fruit and tree) so the total vocabulary size is 2.\n",
        "\n",
        "  :param terms: A list of strings with all unique words that appear in the dataset.\n",
        "  :param freqs_per_class: A dictionary containing the frequency of terms appearing in texts of each class.\n",
        "  :return: A nested dictionary contains the terms and classes with the term's appearance probability.\n",
        "  \n",
        "  \"\"\"\n",
        "  # get list of all classes\n",
        "  classes = freqs_per_class.keys()\n",
        "\n",
        "  # create a default dict so that a new entry is created with the missing key as key and 1/V as value\n",
        "  result_dict = defaultdict(lambda : {c:(1/len(terms)) for c in classes})\n",
        "  \n",
        "  # calculate the probabilities for each class\n",
        "  for c, words in freqs_per_class.items():\n",
        "    \n",
        "    # calculate the probability for a single word\n",
        "    for word in words:\n",
        "      \n",
        "      # apply formula count(w,c) + 1 / count(c) + V\n",
        "      probability = (freqs_per_class[c][word] + 1) / (sum(words.values()) + len(terms))\n",
        "      \n",
        "      # store probability in result dict\n",
        "      result_dict[word][c] = probability\n",
        "  \n",
        "  return result_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeFX353_hFgZ",
        "colab": {}
      },
      "source": [
        "cond_probs = calculate_class_term_probs(terms, freqs_per_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJcQnne8bWPB",
        "colab_type": "code",
        "outputId": "7daad4ad-7fe3-4893-d6fe-f3d651cc3039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(cond_probs[\"wow\"][\"pos\"])\n",
        "print(cond_probs[\"sad\"][\"neg\"])\n",
        "# NOTE: this is a word that does not exist in the data. therefore the value\n",
        "# becomes 1/len(terms) - this is what the formula above for laplace smoothing tells us \n",
        "print(cond_probs[\"pneumonoultramicroscopicsilicovolcanoconiosis\"][\"pos\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.973062865278435e-06\n",
            "0.000111077695447628\n",
            "6.947146112377036e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JRlVMNfrZcQl",
        "colab": {}
      },
      "source": [
        "def classify(text, priors, probs):\n",
        "  \"\"\"\n",
        "\n",
        "  This function should predict a class for a given text.\n",
        "  Use the priors you calculated in 1a).\n",
        "  Use the probabilities you calculated in 1c).\n",
        "\n",
        "  !!!!!!!\n",
        "  Instead of multiplying the probabilites together you can take the logarithm\n",
        "  of the probabilities and multiply them together. \n",
        "  !!!!!!!\n",
        "\n",
        "  Use the Naive Bayes classification to assign the class.\n",
        "  \n",
        "  :param text: A string contains the text to clasiffy.\n",
        "  :param priors: A dictionary mapping the class to its probability (From TASK 1a).\n",
        "  :param probs: A nested dictionary contains the terms and classes with\n",
        "  the term's appearance probability (From TASK 1c).\n",
        "  :return: A string for the class label predicted.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # get list of all classes\n",
        "  classes = priors.keys()\n",
        "\n",
        "  # split the text provided to single words\n",
        "  words = text.split(' ')\n",
        "\n",
        "  # create empty dict to store the probabiliy for each class\n",
        "  probabilities = dict()\n",
        "\n",
        "  # calculate for each class the probability\n",
        "  for c in classes:\n",
        "\n",
        "    # empty list to store all single word probabilities\n",
        "    single_probs = list()\n",
        "\n",
        "    # calculate the probability for all single words\n",
        "    for word in words:\n",
        "\n",
        "      # convert the word to lower-case as it is stored like this in the probs dict\n",
        "      word = word.lower()\n",
        "\n",
        "      # apply Naive Bayes calculation, Px can be ignored\n",
        "      Py = priors[c]\n",
        "      Pxy = probs[word][c]\n",
        "      single_probs.append(math.log(Pxy) * Py)\n",
        "    \n",
        "    # sum the single probabilities to receive the total probability for the whole text for the specific class\n",
        "    probabilities[c] = sum(single_probs)\n",
        "\n",
        "  # return the class with the highest cummulative probability\n",
        "  return max(probabilities, key=probabilities.get)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J09bPDOIg0CZ",
        "outputId": "5c4585f4-9a12-4070-859a-818b1af3ab6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "t1 = \"Today is an awful day.\"\n",
        "pred = classify(t1, priors, cond_probs)\n",
        "print(f\"'{t1}' is {pred}\")\n",
        "t2 = \"Today is a nice day.\"\n",
        "pred = classify(t2, priors, cond_probs)\n",
        "print(f\"'{t2}' is {pred}\")\n",
        "t3 = \"pneumonoultramicroscopicsilicovolcanoconiosis\"\n",
        "pred = classify(t3, priors, cond_probs)\n",
        "print(f\"'{t3}' is {pred}\")\n",
        "t4 = \"This movie was very good and I liked it a lot.\"\n",
        "pred = classify(t4, priors, cond_probs)\n",
        "print(f\"'{t4}' is {pred}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Today is an awful day.' is neg\n",
            "'Today is a nice day.' is pos\n",
            "'pneumonoultramicroscopicsilicovolcanoconiosis' is neg\n",
            "'This movie was very good and I liked it a lot.' is pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMUwhG-5ZfG4",
        "colab": {}
      },
      "source": [
        "def evaluate(test, priors, probs):\n",
        "  \"\"\"\n",
        "  Evaluate your classification.\n",
        "\n",
        "  -> You may assume that the only classes are \"pos\" and \"neg\" ! <--\n",
        "\n",
        "  The function takes a DataFrame to evaluate the classifier from TASK 1d.\n",
        "  For each row in the test dataset, get the actual class from the \"class\" column\n",
        "  and the text from the \"text\" column.\n",
        "  Use your \"classify(text, priors, probs)\" function to get the predicted\n",
        "  class label for each text.\n",
        "\n",
        "  HINT: Count the TP, FP, FN, TN for each of the two classes, for each prediction. \n",
        "\n",
        "  Calculate the following metrics PER CLASS:\n",
        "    - precision\n",
        "    - recall\n",
        "    - f1-measure\n",
        "\n",
        "  :param test: A pandas.DataFrame containing a column \"class\" and a column \"text\".\n",
        "  :param priors: A dictionary mapping the class to its probability (From TASK 1a).\n",
        "  :param probs: A nested dictionary contains the terms and classes with\n",
        "  the term's appearance probability (From TASK 1c).\n",
        "  :return: two data frames\n",
        "    - metrics, which should represent all the key figures for both classes\n",
        "    - df_stat, shich should represent the confusion matrix\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  tn = 0\n",
        "  fn = 0\n",
        "\n",
        "  # run the classifier for each review in the test data-set\n",
        "  for index, row in test.iterrows():\n",
        "    # actual class of review -> ground-truth y\n",
        "    c = row['class']\n",
        "    # predicted class y^\n",
        "    pred_c = classify(row['text'], priors, probs)\n",
        "\n",
        "    # compare predicted class y^ with ground-truth y and track result\n",
        "    if c == 'pos' and pred_c == 'pos':\n",
        "      tp += 1\n",
        "    elif c == 'neg' and pred_c == 'neg':\n",
        "      tn += 1\n",
        "    elif c == 'pos' and pred_c != c:\n",
        "      fn += 1\n",
        "    elif c == 'neg' and pred_c != c:\n",
        "      fp += 1\n",
        "\n",
        "  # calculate metrics for positive class\n",
        "  pos_precision = tp / (tp + fp)\n",
        "  pos_recall = tp / (tp + fn)\n",
        "  pos_f1 = (2 * pos_precision * pos_recall) / (pos_precision + pos_recall)\n",
        "\n",
        "  # calucalte metrics for negative class\n",
        "  neg_precision = tn / (tn + fn)\n",
        "  neg_recall = tn / (tn + fp)\n",
        "  neg_f1 = (2 * neg_precision * neg_recall) / (neg_precision + neg_recall)\n",
        "\n",
        "  # create confusion matrix as DataFrame\n",
        "  df_stat = pd.DataFrame([[tp, fp], [fn, tn]], columns=[['Actual', 'Actual'],['Positive', 'Negative']], index=[['Predicted', 'Predicted'], ['Positive', 'Negative']])\n",
        "\n",
        "  # create DataFrame to visualize the metrics\n",
        "  # bring the data in a order so that it can be included in a DataFrame -> List of Tuples for example\n",
        "  result_list = list()\n",
        "  result_list.append(('pos', pos_precision, pos_recall, pos_f1))\n",
        "  result_list.append(('neg', neg_precision, neg_recall, neg_f1))\n",
        "\n",
        "  # create the DataFrame\n",
        "  metrics = pd.DataFrame(result_list, columns=['Class', 'Precision', 'Recall','F1'])\n",
        "  metrics.set_index('Class', inplace=True)\n",
        "  metrics.index.name = None\n",
        "\n",
        "  return metrics, df_stat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1P37lDUUgvFM",
        "colab": {}
      },
      "source": [
        "metrics, df_stat = evaluate(test_df, priors, cond_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3awJWVFgfnOH",
        "colab_type": "code",
        "outputId": "062f0757-021e-4a9a-f7ec-19a4056b36a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "metrics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pos</th>\n",
              "      <td>0.839234</td>\n",
              "      <td>0.7538</td>\n",
              "      <td>0.794226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neg</th>\n",
              "      <td>0.776547</td>\n",
              "      <td>0.8556</td>\n",
              "      <td>0.814159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Precision  Recall        F1\n",
              "pos   0.839234  0.7538  0.794226\n",
              "neg   0.776547  0.8556  0.814159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an50jGc5c6bR",
        "colab_type": "code",
        "outputId": "7b622a1a-aad9-4a87-b78f-839a6c24bc97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "df_stat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">Actual</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Positive</th>\n",
              "      <th>Negative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Predicted</th>\n",
              "      <th>Positive</th>\n",
              "      <td>3769</td>\n",
              "      <td>722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Negative</th>\n",
              "      <td>1231</td>\n",
              "      <td>4278</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Actual         \n",
              "                   Positive Negative\n",
              "Predicted Positive     3769      722\n",
              "          Negative     1231     4278"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxI6ExAtIB-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calulate TPR and FPR\n",
        "TPR = df_stat.iloc[0, 0] / (df_stat.iloc[0, 0] + df_stat.iloc[1, 0])\n",
        "FPR = df_stat.iloc[0, 1] / (df_stat.iloc[0, 1] + df_stat.iloc[1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}